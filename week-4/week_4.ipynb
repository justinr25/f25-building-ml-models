{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uPOAXdR8SIYJ"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import random\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WUuy3NkjSEnw"
      },
      "outputs": [],
      "source": [
        "### Expressions ###\n",
        "\n",
        "class expression:\n",
        "    \"\"\"Base expression class. Represents a singular variable, like x.\n",
        "    \"\"\"\n",
        "    def __init__(self, name):\n",
        "        \"\"\"Init function.\n",
        "            Args:\n",
        "                Name: name of the variable, like \"x.\"\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "\n",
        "    def eval(self, values):\n",
        "        \"\"\"Evaluation function, returns value of expression/variable given value parameters.\n",
        "\n",
        "        Args:\n",
        "            values (dict): a dicitonary of variable names:values.\n",
        "\n",
        "        Returns:\n",
        "            float representing value of variable at given evaluation.\n",
        "        \"\"\"\n",
        "        return values[self.name]\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        \"\"\"Differentiate variable / expression with the current values.\n",
        "\n",
        "        Args:\n",
        "            values (dict): a dicitonary of variable names:values.\n",
        "            diffto (string): variable with respect to which differentiation happens.\n",
        "\n",
        "        Returns:\n",
        "            1 or 0 -- depends if this is the variable we're differentiating with respect to.\n",
        "        \"\"\"\n",
        "        return 1 if diffto == self.name else 0\n",
        "\n",
        "    # the following are all overrides: (all other subsequent classes inherit)\n",
        "    def __add__(self, other):\n",
        "        if isinstance(other, (int, float)):\n",
        "            other = constant(other)\n",
        "        return addition(self, other)\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        if isinstance(other, (int, float)):\n",
        "            other = constant(other)\n",
        "        return addition(other, self)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if isinstance(other, (int, float)):\n",
        "            other = constant(other)\n",
        "        return subtraction(self, other)\n",
        "\n",
        "    def __rsub__(self, other):\n",
        "        if isinstance(other, (int, float)):\n",
        "            other = constant(other)\n",
        "        return subtraction(other, self)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        if isinstance(other, (int, float)):\n",
        "            other = constant(other)\n",
        "        return multiplication(self, other)\n",
        "\n",
        "    def __rmul__(self, other):\n",
        "        if isinstance(other, (int, float)):\n",
        "            other = constant(other)\n",
        "        return multiplication(other, self)\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        if isinstance(other, (int, float)):\n",
        "            other = constant(other)\n",
        "        return division(self, other)\n",
        "\n",
        "    def __rtruediv__(self, other):\n",
        "        if isinstance(other, (int, float)):\n",
        "            other = constant(other)\n",
        "        return division(other, self)\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        if isinstance(other, (int, float)):\n",
        "            other = constant(other)\n",
        "        return exponent(self, other)\n",
        "\n",
        "    def __rpow__(self, other):\n",
        "        if isinstance(other, (int, float)):\n",
        "            other = constant(other)\n",
        "        return exponent(other, self)\n",
        "\n",
        "class constant(expression):\n",
        "    \"\"\"Constant expression class. Represents a constant value, like 2 or 3.14.\n",
        "    \"\"\"\n",
        "    def __init__(self, value):\n",
        "        \"\"\"Init function.\n",
        "            Args:\n",
        "                value: numeric value of the constant.\n",
        "        \"\"\"\n",
        "        self.value = value\n",
        "\n",
        "    def eval(self, values):\n",
        "        \"\"\"Evaluation function, returns the constant value.\n",
        "\n",
        "        Args:\n",
        "            values (dict): a dicitonary of variable names:values (not used for constants).\n",
        "\n",
        "        Returns:\n",
        "            float representing the constant value.\n",
        "        \"\"\"\n",
        "        return self.value\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        \"\"\"Differentiate constant with respect to any variable.\n",
        "\n",
        "        Args:\n",
        "            values (dict): a dicitonary of variable names:values (not used).\n",
        "            diffto (string): variable with respect to which differentiation happens (not used).\n",
        "\n",
        "        Returns:\n",
        "            0 -- derivative of a constant is always 0.\n",
        "        \"\"\"\n",
        "        return 0\n",
        "\n",
        "class addition(expression):\n",
        "    def __init__(self, first, second):\n",
        "        \"\"\"Addition of two expressions.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "            second (expression): another expression\n",
        "        \"\"\"\n",
        "        self.first = first\n",
        "        self.second = second\n",
        "\n",
        "    def eval(self, values):\n",
        "        return self.first.eval(values) + self.second.eval(values)\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        return self.first.diff(values, diffto) + self.second.diff(values, diffto)\n",
        "\n",
        "class subtraction(expression):\n",
        "    def __init__(self, first, second):\n",
        "        \"\"\"Subtraction of two expressions.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "            second (expression): another expression\n",
        "        \"\"\"\n",
        "        self.first = first\n",
        "        self.second = second\n",
        "\n",
        "    def eval(self, values):\n",
        "        return self.first.eval(values) - self.second.eval(values)\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        return self.first.diff(values, diffto) - self.second.diff(values, diffto)\n",
        "\n",
        "class multiplication(expression):\n",
        "    def __init__(self, first, second):\n",
        "        \"\"\"Multiplication of two expressions.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "            second (expression): another expression\n",
        "        \"\"\"\n",
        "        self.first = first\n",
        "        self.second = second\n",
        "\n",
        "    def eval(self, values):\n",
        "        return self.first.eval(values) * self.second.eval(values)\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        # product rule, u'(x) * v(x) + u(x) * v'(x)\n",
        "        return self.first.diff(values, diffto) * self.second.eval(values) + self.first.eval(values) * self.second.diff(values, diffto)\n",
        "\n",
        "class division(expression):\n",
        "    def __init__(self, first, second):\n",
        "        \"\"\"Division of two expressions.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "            second (expression): another expression\n",
        "        \"\"\"\n",
        "        self.first = first\n",
        "        self.second = second\n",
        "\n",
        "    def eval(self, values):\n",
        "        return self.first.eval(values) / self.second.eval(values)\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        # quotient rule: (v(x)u'(x) - v'(x)u(x)) / (v(x) ^ 2)\n",
        "        return (self.second.eval(values) * self.first.diff(values, diffto) - self.second.diff(values, diffto) * self.first.eval(values)) / (self.second.eval(values) ** 2)\n",
        "\n",
        "class exponent(expression):\n",
        "    def __init__(self, first, second):\n",
        "        \"\"\"Exponent of two expressions.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "            second (expression): another expression\n",
        "        \"\"\"\n",
        "        self.first = first\n",
        "        self.second = second\n",
        "\n",
        "    def eval(self, values):\n",
        "        return self.first.eval(values) ** self.second.eval(values)\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        # power rule -- generalized\n",
        "        return self.eval(values) * (natlog(self.first) * self.second).diff(values, diffto)\n",
        "\n",
        "### Functions -- single input###\n",
        "# These MUST account for the chain rule.\n",
        "# They also aren't called via overrides, meaning that they should handle casts to constant if need be.\n",
        "class natlog(expression):\n",
        "    def __init__(self, first):\n",
        "        \"\"\"Natural log of an expression.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "        \"\"\"\n",
        "        self.first = constant(first) if isinstance(first, (int, float)) else first\n",
        "\n",
        "    def eval(self, values):\n",
        "        return math.log(self.first.eval(values))\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        # chain rule\n",
        "        return 1 / self.first.eval(values) * self.first.diff(values, diffto)\n",
        "\n",
        "#class sine\n",
        "class sine(expression):\n",
        "    def __init__(self, first):\n",
        "        \"\"\"sine of an expression.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "        \"\"\"\n",
        "        self.first = constant(first) if isinstance(first, (int, float)) else first\n",
        "\n",
        "    def eval(self, values):\n",
        "        return math.sin(self.first.eval(values))\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        # chain rule\n",
        "        # debatable to use math cos here instead of native. Should just have same effect but faster.\n",
        "        return math.cos(self.first.eval(values)) * self.first.diff(values, diffto)\n",
        "\n",
        "#class cosine\n",
        "class cosine(expression):\n",
        "    def __init__(self, first):\n",
        "        \"\"\"cosine of an expression.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "        \"\"\"\n",
        "        self.first = constant(first) if isinstance(first, (int, float)) else first\n",
        "\n",
        "    def eval(self, values):\n",
        "        return math.cos(self.first.eval(values))\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        # chain rule\n",
        "        return -1 * math.sin(self.first.eval(values)) * self.first.diff(values, diffto)\n",
        "\n",
        "#class tangent\n",
        "class tangent(expression):\n",
        "    def __init__(self, first):\n",
        "        \"\"\"tangent of an expression.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "        \"\"\"\n",
        "        self.first = constant(first) if isinstance(first, (int, float)) else first\n",
        "\n",
        "    def eval(self, values):\n",
        "        return math.tan(self.first.eval(values))\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        # chain rule\n",
        "        return 1 / math.cos(self.first.eval(values)) ** 2 * self.first.diff(values, diffto)\n",
        "\n",
        "#class tanh (tanh)\n",
        "class hyperbolic_tan(expression):\n",
        "    def __init__(self, first):\n",
        "        \"\"\"tanh. activation function\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "        \"\"\"\n",
        "        self.first = constant(first) if isinstance(first, (int, float)) else first\n",
        "\n",
        "    def eval(self, values):\n",
        "        return math.tanh(self.first.eval(values))\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        return 1 / math.cosh(self.first.eval(values)) ** 2 * self.first.diff(values, diffto)\n",
        "\n",
        "#class sigmoid\n",
        "class sigmoid(expression):\n",
        "    def __init__(self, first):\n",
        "        \"\"\"Sigmoid activation function.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "        \"\"\"\n",
        "        self.first = constant(first) if isinstance(first, (int, float)) else first\n",
        "\n",
        "    def eval(self, values):\n",
        "        return 1 / (1 + math.e ** (-1 * self.first.eval(values)))\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        # chain rule\n",
        "        return math.e ** self.first.eval(values) / (math.e ** self.first.eval(values) + 1) ** 2 * self.first.diff(values, diffto)\n",
        "\n",
        "#class relu\n",
        "class relu(expression):\n",
        "    def __init__(self, first):\n",
        "        \"\"\"Rectified linear unit activation function.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "        \"\"\"\n",
        "        self.first = constant(first) if isinstance(first, (int, float)) else first\n",
        "\n",
        "    def eval(self, values):\n",
        "        return max(self.first.eval(values), 0)\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        # TEST: change this to just 1/0. See if it breaks an intermediate test.\n",
        "        return self.first.diff(values, diffto) if self.eval(values) > 0 else 0\n",
        "\n",
        "#class relu_leaky\n",
        "class leaky_relu(expression):\n",
        "    def __init__(self, first):\n",
        "        \"\"\"Leaky rectified linear unit activation function.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "        \"\"\"\n",
        "        self.first = constant(first) if isinstance(first, (int, float)) else first\n",
        "\n",
        "    def eval(self, values):\n",
        "        evaluated = self.first.eval(values)\n",
        "        return  evaluated if evaluated > 0 else 0.001\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        # TEST: change this to just 1/0. See if it breaks an intermediate test.\n",
        "        return self.first.diff(values, diffto) if self.eval(values) > 0 else 0.001 * self.first.diff(values, diffto)\n",
        "\n",
        "#class linear_activation\n",
        "class linear(expression):\n",
        "    def __init__(self, first):\n",
        "        \"\"\"Linear activation function.\n",
        "\n",
        "        Args:\n",
        "            first (expression): an expression\n",
        "        \"\"\"\n",
        "        self.first = constant(first) if isinstance(first, (int, float)) else first\n",
        "\n",
        "    def eval(self, values):\n",
        "        return self.first.eval(values)\n",
        "\n",
        "    def diff(self, values, diffto):\n",
        "        # chain rule\n",
        "        return self.first.diff(values, diffto)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "59aeh1Yhf8sT"
      },
      "outputs": [],
      "source": [
        "# linear regression using the autodiff library\n",
        "class mdst_linear_reg():\n",
        "    \"\"\"Multivariable linear regression (MLR) for MDST project, \"Building ML Models.\"\n",
        "\n",
        "    Attributes:\n",
        "        mx: Slopes of multivariate linear regression\n",
        "        yint: Y-intercept of multivariable linear regression\n",
        "\n",
        "    Methods:\n",
        "        fit(X_fit, y_fit): Using gradient descent, tune slope attributes & y-intercept over n_epochs.\n",
        "        predict(X_pred): Using pre-tuned slope, y-intercept, estimate theoretical y-values for input X_pred.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializer function. Create slope and y-intercept attributes, blank.\n",
        "\n",
        "        Args:\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        self.mx = {}\n",
        "        self.yint = 0\n",
        "\n",
        "    def fit(self, X_fit, y_fit, n_epochs, learning_rate=0.01):\n",
        "        \"\"\"Fit MLR on training data defined by X_fit, y_fit over n_epochs.\n",
        "\n",
        "        Args:\n",
        "            X_fit (pd.dataframe): x values / features in training data.\n",
        "                Number of columns defines num. slopes in mx attribute, number of rows MUST be same as length of y_fit.\n",
        "\n",
        "            y_fit (list-like): y values / target in training data.\n",
        "                Number of values MUST be same as number of rows in X_fit.\n",
        "\n",
        "            n_epochs (int): number of training epochs, or number of times MLR iterates through the full x-y training set.\n",
        "\n",
        "            learning_rate (float): step size for gradient descent updates.\n",
        "        \"\"\"\n",
        "        # step 1: populate the mx dictionary with X_columns number of slopes\n",
        "        for i in range(len(X_fit.columns)):\n",
        "            self.mx['m' + str(i)] = random.uniform(-1, 1) # NOT expressions; these are numeric values.\n",
        "\n",
        "        # Initialize y-intercept\n",
        "        self.yint = random.uniform(-1, 1)\n",
        "\n",
        "        # step 2: Set up autodiff expressions for parameters\n",
        "        # Create expression variables for each slope and y-intercept\n",
        "        slope_vars = {}\n",
        "        for i in range(len(X_fit.columns)):\n",
        "            slope_vars['m' + str(i)] = expression('m' + str(i))\n",
        "        yint_var = expression('yint')\n",
        "\n",
        "        # step 3: Gradient descent training\n",
        "        for epoch in range(n_epochs):\n",
        "\n",
        "            # Initialize gradients for this epoch\n",
        "            # Gradients is a dictionary to store the gradients for each parameter (slopes and y-intercept)\n",
        "            # At the conclusion of this epoch, these will be used to update the parameters.\n",
        "            gradients = {}\n",
        "            for key in self.mx.keys():\n",
        "                gradients[key] = 0\n",
        "            gradients['yint'] = 0\n",
        "\n",
        "            # Process each training example\n",
        "            for idx in range(len(X_fit)):\n",
        "\n",
        "                # Prediction expression: single prediction for each row of the training data.\n",
        "                prediction_expr = 0\n",
        "                # TODO: complete prediction_expr construction using slope_vars, X_fit, and yint_var values.\n",
        "                # Hint: Build the prediction expression using the formula of a line: y_pred = m0*x0 + m1*x1 + ... + yint\n",
        "                for i in range(len(X_fit.columns)):\n",
        "                    prediction_expr += slope_vars['m' + str(i)] * X_fit.iloc[idx, i]\n",
        "                prediction_expr += yint_var\n",
        "\n",
        "                # Create loss expression: (y_true - y_hat)^2\n",
        "                y_true = y_fit[idx]\n",
        "                # Use manual squaring to avoid domain error with math.log(): (a-b)^2 = (a-b)*(a-b)\n",
        "                diff_expr = y_true - prediction_expr\n",
        "                loss_expr = diff_expr * diff_expr\n",
        "\n",
        "                # Current parameter values for evaluation\n",
        "                current_values = dict(self.mx)\n",
        "                current_values['yint'] = self.yint\n",
        "\n",
        "                # Compute gradients\n",
        "                for key in self.mx.keys():\n",
        "                    # TODO: compute gradient for each slope parameter using .diff() with the overall loss expression, loss_expr.\n",
        "                    # Hint: call diff with (current_values, key) to select the correct parameter. Make sure to add this gradient to gradients[key].\n",
        "                    gradients[key] += loss_expr.diff(current_values, key)\n",
        "\n",
        "                # TODO: compute gradient for y-intercept using .diff() with loss_expr, with respect to 'yint'.\n",
        "                gradients['yint'] += loss_expr.diff(current_values, 'yint')\n",
        "\n",
        "            # Update parameters using gradient descent\n",
        "            for key in self.mx.keys():\n",
        "                pass\n",
        "                # TODO: complete parameter update step using learning_rate and gradients.\n",
        "                # Hint: you have self.mx populated with slope values -- update each slope here using the computed gradient values.\n",
        "                self.mx[key] -= gradients[key] * learning_rate\n",
        "            # TODO: Once this is complete, do so for the y-intercept as well.\n",
        "            self.yint -= gradients['yint'] * learning_rate\n",
        "\n",
        "\n",
        "    def predict(self, X_pred):\n",
        "        \"\"\"Generate prediction / hypothetical y based on X input, using pre-trained slope and y-intercept.\n",
        "\n",
        "        Args:\n",
        "            X_pred (pd.dataframe): x values / features in prediction data.\n",
        "\n",
        "        Returns:\n",
        "            list: predicted y values for each row in X_pred.\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for idx in range(len(X_pred)):\n",
        "            # y = m0*x0 + m1*x1 + ... + yint\n",
        "            prediction = self.yint\n",
        "\n",
        "            # TODO: complete prediction calculation.\n",
        "            # Hint: you now have self.mx populated with slope values -- you don't need to use autodiff expressions here!\n",
        "            # (Do something with the variable \"prediction\"... the first step of computation is done for you)\n",
        "            # for key in self.mx.keys():\n",
        "            for i in range(len(X_pred.columns)):\n",
        "                prediction += self.mx['m' + str(i)] * X_pred.iloc[idx, i]\n",
        "\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        return predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TWpvRmHqgEWB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MDST Linear Regression with Autodiff Demo ===\n",
            "\n",
            "Created synthetic dataset:\n",
            "  Samples: 100\n",
            "  Features: ['feature1', 'feature2']\n",
            "  True parameters: slope1=2, slope2=3, intercept=1\n",
            "\n",
            "Train set: 80 samples\n",
            "Test set: 20 samples\n",
            "\n",
            "Training model with gradient descent...\n",
            "\n",
            "Test Mean Squared Error: 0.008725\n",
            "\n",
            "Sample predictions vs actual:\n",
            "Predicted | Actual\n",
            "--------------------\n",
            "   2.443 |  2.449\n",
            "  -0.849 | -0.791\n",
            "   0.775 |  0.902\n",
            "   1.409 |  1.287\n",
            "  -1.301 | -1.074\n",
            "   2.138 |  1.943\n",
            "   4.281 |  4.235\n",
            "   1.452 |  1.498\n",
            "  -2.611 | -2.572\n",
            "  -2.511 | -2.580\n",
            "\n",
            "Model successfully trained using autodiff for gradient computation!\n"
          ]
        }
      ],
      "source": [
        "# Once you're done with the actual coding portions, run this code to demonstrate your model.\n",
        "# Courtesy of Github Copilot.\n",
        "def main():\n",
        "    \"\"\"Demonstration of the linear regression model using gradient descent with autodiff.\"\"\"\n",
        "    print(\"=== MDST Linear Regression with Autodiff Demo ===\\n\")\n",
        "\n",
        "    # Create synthetic data for demonstration\n",
        "    import numpy as np\n",
        "\n",
        "    # Generate synthetic dataset: y = 2*x1 + 3*x2 + 1 + noise\n",
        "    np.random.seed(42)  # for reproducible results\n",
        "    n_samples = 100\n",
        "\n",
        "    x1 = np.random.normal(0, 1, n_samples)\n",
        "    x2 = np.random.normal(0, 1, n_samples)\n",
        "    noise = np.random.normal(0, 0.1, n_samples)\n",
        "    y = 2 * x1 + 3 * x2 + 1 + noise\n",
        "\n",
        "    # Create DataFrame\n",
        "    X = pd.DataFrame({\n",
        "        'feature1': x1,\n",
        "        'feature2': x2\n",
        "    })\n",
        "\n",
        "    print(\"Created synthetic dataset:\")\n",
        "    print(f\"  Samples: {n_samples}\")\n",
        "    print(f\"  Features: {X.columns.tolist()}\")\n",
        "    print(f\"  True parameters: slope1=2, slope2=3, intercept=1\")\n",
        "    print()\n",
        "\n",
        "    # Split data for training and testing\n",
        "    split_idx = int(0.8 * n_samples)\n",
        "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "    print(f\"Train set: {len(X_train)} samples\")\n",
        "    print(f\"Test set: {len(X_test)} samples\")\n",
        "    print()\n",
        "\n",
        "    # Create and train model\n",
        "    model = mdst_linear_reg()\n",
        "\n",
        "    print(\"Training model with gradient descent...\")\n",
        "    model.fit(X_train, y_train, n_epochs=1000, learning_rate=0.01)\n",
        "    print()\n",
        "\n",
        "    # Make predictions on test set\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Calculate test error\n",
        "    test_error = sum((y_test[i] - predictions[i])**2 for i in range(len(y_test))) / len(y_test)\n",
        "    print(f\"Test Mean Squared Error: {test_error:.6f}\")\n",
        "\n",
        "    # Show some predictions vs actual\n",
        "    print(\"\\nSample predictions vs actual:\")\n",
        "    print(\"Predicted | Actual\")\n",
        "    print(\"-\" * 20)\n",
        "    for i in range(min(10, len(predictions))):\n",
        "        print(f\"{predictions[i]:8.3f} | {y_test[i]:6.3f}\")\n",
        "\n",
        "    print(f\"\\nModel successfully trained using autodiff for gradient computation!\")\n",
        "if __name__ == \"__main__\":\n",
        "     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mdst",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
